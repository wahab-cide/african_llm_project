model:
  vocab_size: 16003
  n_layer: 2
  n_embd: 256
  n_head: 4
  bos_token_id: 1
  eos_token_id: 2
  pad_token_id: 3

data:
  tokenized_path: "data/processed/tokenized_dataset_small"
  max_seq_len: 128

training:
  batch_size: 8
  gradient_accum: 4
  lr: 2e-4
  warmup_ratio: 0.06
  fp16: false
  num_epochs: 3
  logging_steps: 100

logging:
  project: "african-llm"
  run_id: "htf-v1-fast" 