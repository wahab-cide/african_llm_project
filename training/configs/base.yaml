model:
  arch: "gpt-mini"
  vocab_size: 16003
  n_layer: 4
  n_embd: 384
  n_head: 6
  bos_token_id: 1
  eos_token_id: 2
  pad_token_id: 3

data:
  tokenized_path: "data/processed/tokenized_dataset"
  max_seq_len: 256

training:
  batch_size: 16
  gradient_accum: 2
  lr: 2e-4
  warmup_ratio: 0.06
  fp16: true
  num_epochs: 5
  logging_steps: 50

logging:
  project: "african-llm"
  run_id: "htf-v1" 